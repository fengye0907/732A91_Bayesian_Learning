knitr::opts_chunk$set(echo = TRUE, out.width = "400px")
# packages we use in the lab
library(mvtnorm)
library(LaplacesDemon)
library(ggplot2)
library(reshape2)
library(gridExtra)
# clean environment
rm(list=ls())
# load the data
tempdata <- read.table("TempLinkoping.txt",header = TRUE, sep = "\t")
y        <- tempdata$temp
# lecture 5 slide 5 -  The linear regression model in matrix form
# X - matrix represents the model
X        <- cbind(1, tempdata$time, tempdata$time**2)
n        <- nrow(X)
# given parameter values
mu_0     <- c(-10, 100, -100)
omega_0  <- 0.01 * diag(3)
v_0      <- 4
sigma2_0 <- 1
# linear model - without beta
lmTemp = lm( temp ~ time + I(time^2), data = tempdata)
summary(lmTemp)
# plot the data
ggplot(tempdata, aes(x = time, y=temp)) +
geom_point(aes(color = "#e74c3c")) +                         # default blue
ylab("Temperature") + xlab("Time") + ggtitle("Time vs Temperature") +
scale_color_identity(name = "colour",
breaks = c("#e74c3c"),
labels = c("Observations"),
guide = "legend") +
theme(legend.position = "bottom")
# create simulation
set.seed(123456)
# joint conjugate prior
N <- 40  # nr of simulations
# data frame to save prior coef data
prior <- matrix(ncol = 3, nrow = N)
for (i in 1:N) {
# lecture 5 slide 7 - Inv - chi^2(v_0,sigma^2_0)
var  <- LaplacesDemon::rinvchisq(1 ,v_0, sigma2_0)
# solve(A)	Inverse of A where A is a square matrix
beta <- MASS::mvrnorm(1, mu_0, var*solve(omega_0))
prior[i,1:3] <- beta
}
data.1a <- as.data.frame(cbind(tempdata$time, X%*%t(prior)))
cnames <- c("x")
for (i in 1:N) {
cnames[1+i] <- paste0("pred.",i)
}
colnames(data.1a) <- cnames
data.1a <- melt(data.1a, id.vars = "x") # data.1a - variable to create a color for each line
plot1a.ori <- ggplot(data.1a)+
geom_line(aes(x = x, y = value, color = variable)) +
geom_point(data = tempdata, aes(x = time, y = temp), colour = "#e74c3c") +
ggtitle("Original parameters") + ylab("Temperature") + xlab("Time") +
theme(legend.position = "none")
# modify the parameters
mu_0     <- c(-10.68, 93.60, -85.83)  # before c(-10, 100, -100),
# change into values given in the lm output
omega_0  <- 0.01 * diag(3)
v_0      <- 4
sigma2_0 <- 0.02              # before 1 - change
# -------------------------- same code as before - maybe create a function
prior <- matrix(ncol = 3, nrow = N)
for (i in 1:N) {
var  <- LaplacesDemon::rinvchisq(1 ,v_0, sigma2_0)
beta <- MASS::mvrnorm(1, mu_0, var*solve(omega_0))
prior[i,1:3] <- beta
}
# Linear regression in matrix form - X%*%t(prior)
# lecture 5 slide 5
data.1a <- as.data.frame(cbind(tempdata$time, X %*% t(prior)))
cnames <- c("x")
for (i in 1:N) {
cnames[1+i] <- paste0("pred.",i)
}
colnames(data.1a) <- cnames
data.1a <- melt(data.1a, id.vars = "x")
plot1a.mdf <- ggplot(data.1a)+
geom_line(aes(x = x, y = value, color = variable)) +
geom_point(data = tempdata, aes(x = time, y = temp), color = "#e74c3c") +
ggtitle("Modified parameter") + ylab("Temperature") + xlab("Time") +
theme(legend.position = "none")
plot(arrangeGrob(plot1a.ori, plot1a.mdf, nrow = 2))
# clean up environment
rm(list=ls())
# clean up environment
rm(list=ls())
knitr::include_graphics("lab2_table_ex2.png")
# load the data & define y (response) and x 8 dimensional vector of the features
WomenWork = read.table("WomenWork.dat", header = TRUE)
y = WomenWork$Work
X = WomenWork[,2:9]
X = as.matrix(X)
n = nrow(WomenWork) # nr of observations
# logistic regression
glmModel <- glm(Work ~ 0 + ., data = WomenWork, family = binomial)
summary(glmModel)
# parameters
mu  <- 0
# Prior scaling factor such that Prior Covariance = (tau^2)*I
#- need tau to define sigma later on
tau <- 10
# rename X columns
covNames <- names(WomenWork)[2:length(names(WomenWork))]
m <- dim(X)[2]
# setting up the prior
mu    <- as.vector(rep(mu,m))  # Prior mean vector
Sigma <- tau^2 * diag(m)  # Prior variance matrix
LogPostLogistic <- function(beta, y, X, mu, Sigma){
M <- length(beta)
p <- X %*% beta
# log the likelihood of Binomial Distribution
logLik <- sum(p * y - log(1 + exp(p)))
if (abs(logLik) == Inf){
logLik <- -50000
# Likelihood is not finite, stear the optimizer away from here!
# by Teacher's idea
}
# prior follows multi-normal distribution with
logPrior <- dmvnorm(beta, mu, Sigma, log = TRUE)
# cuz we logarithmize the likelihood and prior,
# posterior is the sum of them
return(logLik + logPrior)
}
# use random initial values
initVal <- as.vector(rnorm(dim(X)[2]))
OptimResults <- optim(initVal,
LogPostLogistic,
y = y,
X = X,
mu = mu,
Sigma = Sigma,
method = c("BFGS"),
control = list(fnscale=-1),
hessian = TRUE)  # output hessian matrix
# Printing the results to the screen
postMode <- OptimResults$par
# Posterior covariance matrix is -inv(Hessian)
postCov <- -solve(OptimResults$hessian)
# Computing approximate standard deviations.
approxPostStd <- sqrt(diag(postCov))
# Naming the coefficient by covariates
names(postMode) <- covNames
names(approxPostStd) <- covNames
print('The posterior mode is:')
print(postMode)
print('The approximate posterior standard deviation is:')
print(approxPostStd)
# Compute an approximate 95% credible interval for the variable NSmallChild
# By simulation
data.NSmallChild <- as.data.frame(
mvtnorm::rmvnorm(n = 1000, mean = postMode, sigma = postCov)
)[,7]
# 95% credible interval
quantile(data.NSmallChild, probs = c(0.025,0.975))
CI_0025 = quantile(data.NSmallChild, probs = c(0.025,0.975))[1]
CI_0975 = quantile(data.NSmallChild, probs = c(0.025,0.975))[2]
# By mean and standard deviation
# 1.96 - T-Distribution
interval <- c(postMode[7] - 1.96 * approxPostStd[7], postMode[7] + 1.96 * approxPostStd[7])
names(interval) <- NULL
cat(interval)
###################################################################################
# Author: Mattias Villani, Linkoping University.
#         E-mail: mattias.villani@liu.se
#         web: http://mattiasvillani.com
# Script to illustrate numerical maximization of the Logistic or Probit regression
###################################################################################
###########   BEGIN USER INPUTS   ################
Probit <- 0           # If Probit <-0, then logistic model is used.
chooseCov <- c(1:16)  # Here we choose which covariates to include in the model
tau <- 10000;         # Prior scaling factor such that Prior Covariance = (tau^2)*I
###########     END USER INPUT    ################
#install.packages("mvtnorm") # Loading a package that contains the multivariate normal pdf
library("mvtnorm") # This command reads the mvtnorm package into R's memory. NOW we can use dmvnorm function.
# Loading data from file
Data<-read.table("SpamReduced.dat",header=TRUE)  # Spam data from Hastie et al.
# 1000 simulations of beta
data.hist <- as.data.frame(
mvtnorm::rmvnorm(n = 1000, mean = postMode, sigma = postCov)
)
colnames(data.hist) <- covNames
f <- function(cname){
ggplot(data.hist, aes_string(x=cname)) +
geom_histogram(aes(y = ..density..),
colour = "black",
fill   = "white",
bins   = 30) +
geom_density(alpha = .2, fill = "#FF6666")
}
plot(arrangeGrob(grobs = lapply(covNames, f)))
sigmoid_function = function(x){
# this function returns create prediction from a regression (pred.hist) + usesed sigmoid function to
# create probabilites of that to some a classification problem (sigmoid)
#
# Arguments:
#   x = input to the function (your algorithm’s prediction e.g. mx + b)
#
# Returns:
#   data frame - s(x) = output between 0 and 1 (probability estimate) + flags
#
# Flags:
#   50% Decision Boundary
#
# Lecture 6 slide 4
# Classification with logistic regression - Logistic regression
# -------- prediction --------
# 1000 predictions for such example - pred.hist represents X
pred.hist <<- as.data.frame(t(example %*% t(data.hist)))
# comment:
# we used mvtnorm to simulate data.hist, so we use it at this point again
# instead of simulating again
# -------- sigmoid function --------
sigmoid <- as.data.frame(1/(1+exp(-pred.hist)))
colnames(sigmoid) <- "Probability"
# create flags
sigmoid$job_flag <- ifelse(sigmoid$Probability < 0.5, "no_job", "job")
sigmoid$job_flag_binary <- ifelse(sigmoid$Probability < 0.5, 0, 1)
# add simulation nr
sigmoid$nr <- c(1:nrow(sigmoid))
# return the output
sigmoid <<- sigmoid
#return(sigmoid)
}
sigmoid_function(example)
# the given values for the example
example <- matrix(c(1, 10, 8, 10, (10/10)^2, 40, 1, 1), nrow = 1)
sigmoid_function = function(x){
# this function returns create prediction from a regression (pred.hist) + usesed sigmoid function to
# create probabilites of that to some a classification problem (sigmoid)
#
# Arguments:
#   x = input to the function (your algorithm’s prediction e.g. mx + b)
#
# Returns:
#   data frame - s(x) = output between 0 and 1 (probability estimate) + flags
#
# Flags:
#   50% Decision Boundary
#
# Lecture 6 slide 4
# Classification with logistic regression - Logistic regression
# -------- prediction --------
# 1000 predictions for such example - pred.hist represents X
pred.hist <<- as.data.frame(t(example %*% t(data.hist)))
# comment:
# we used mvtnorm to simulate data.hist, so we use it at this point again
# instead of simulating again
# -------- sigmoid function --------
sigmoid <- as.data.frame(1/(1+exp(-pred.hist)))
colnames(sigmoid) <- "Probability"
# create flags
sigmoid$job_flag <- ifelse(sigmoid$Probability < 0.5, "no_job", "job")
sigmoid$job_flag_binary <- ifelse(sigmoid$Probability < 0.5, 0, 1)
# add simulation nr
sigmoid$nr <- c(1:nrow(sigmoid))
# return the output
sigmoid <<- sigmoid
#return(sigmoid)
}
sigmoid_function(example)
ggplot(data = sigmoid, aes(x = Probability)) +
geom_histogram(aes(y = ..density..),
colour = "black",
fill   = "white",
bins   = 30) +
geom_density(alpha = .2, fill = "#FF6666") +
ggtitle("Density - Histogram of work probabily") +
xlim(c(0,1))
sigmoid$job_flag_binary
sigmoid$job_flag
ggplot(data = sigmoid, aes(x = job_flag_binary)) +
geom_histogram(aes(y = ..density..),
colour = "black",
fill   = "white",
bins   = 30) +
geom_density(alpha = .2, fill = "#FF6666") +
ggtitle("Density - Histogram of work probabily") +
xlim(c(0,1))
hist(sigmoid$job_flag_binary )
hist(sigmoid$job_flag_binary,2)
sigmoid$job_flag_binary>0
sum(sigmoid$job_flag_binary>0)
Probability
hist(sigmoid$job_flag_binary,2)
ggplot(data = sigmoid, aes(x = Probability)) +
geom_histogram(aes(y = ..density..),
colour = "black",
fill   = "white",
bins   = 30) +
geom_density(alpha = .2, fill = "#FF6666") +
ggtitle("Density - Histogram of work probabily") +
xlim(c(0,1))
ggplot(data = sigmoid, aes(x = sigmoid$Probability, y = sigmoid$job_flag_binary)) +
xlim(c(0,1)) + ylab("Classification") + xlab("Probability") +
ggtitle("Sigmoid function with 0.5 as decision boundary") +
geom_vline(aes(xintercept = 0.5, color = "#e67e22"), alpha = 0.5) +
geom_hline(aes(yintercept = 0.5, color = "#e67e22"), alpha = 0.5) +
stat_smooth(method="glm", aes(color = "Blue"), alpha = 0.5, se=FALSE, fullrange=TRUE,
method.args = list(family=binomial)) +
geom_point(aes(color = "#e74c3c")) +
scale_color_identity(name = "Colour",
breaks = c("#e74c3c","#e67e22", "Blue"),
labels = c("Observations", "Decision boundary" , "Logisic Regression"),
guide = "legend") +
theme(legend.position="bottom")
ggplot(pred.hist, aes(x=V1)) +
geom_density(alpha = .2, fill = "#FF6666") +
xlab("NSmallChild") + ggtitle("Density and credible interval of the feature NSmallChild ") +
geom_vline(aes(xintercept = CI_0975)) +
geom_vline(aes(xintercept = CI_0025))+
geom_text(aes(label ="97.5 % interval", x =  CI_0975 +0.3, y = 1),size = 3 )+
geom_text(aes(label ="2.5 % interval", x =  CI_0025 -0.3, y = 1),size = 3 )
knitr::include_graphics("lab2_table_ex2.png")
# clean up environment
rm(list=ls())
knitr::include_graphics("lab2_table_ex2.png")
# load the data & define y (response) and x 8 dimensional vector of the features
WomenWork = read.table("WomenWork.dat", header = TRUE)
y = WomenWork$Work
X = WomenWork[,2:9]
X = as.matrix(X)
n = nrow(WomenWork) # nr of observations
# logistic regression
glmModel <- glm(Work ~ 0 + ., data = WomenWork, family = binomial)
summary(glmModel)
# parameters
mu  <- 0
# Prior scaling factor such that Prior Covariance = (tau^2)*I
#- need tau to define sigma later on
tau <- 10
# rename X columns
covNames <- names(WomenWork)[2:length(names(WomenWork))]
m <- dim(X)[2]
# setting up the prior
mu    <- as.vector(rep(mu,m))  # Prior mean vector
Sigma <- tau^2 * diag(m)  # Prior variance matrix
LogPostLogistic <- function(beta, y, X, mu, Sigma){
M <- length(beta)
p <- X %*% beta
# log the likelihood of Binomial Distribution
logLik <- sum(p * y - log(1 + exp(p)))
if (abs(logLik) == Inf){
logLik <- -50000
# Likelihood is not finite, stear the optimizer away from here!
# by Teacher's idea
}
# prior follows multi-normal distribution with
logPrior <- dmvnorm(beta, mu, Sigma, log = TRUE)
# cuz we logarithmize the likelihood and prior,
# posterior is the sum of them
return(logLik + logPrior)
}
# use random initial values
initVal <- as.vector(rnorm(dim(X)[2]))
OptimResults <- optim(initVal,
LogPostLogistic,
y = y,
X = X,
mu = mu,
Sigma = Sigma,
method = c("BFGS"),
control = list(fnscale=-1),
hessian = TRUE)  # output hessian matrix
# Printing the results to the screen
postMode <- OptimResults$par
# Posterior covariance matrix is -inv(Hessian)
postCov <- -solve(OptimResults$hessian)
# Computing approximate standard deviations.
approxPostStd <- sqrt(diag(postCov))
# Naming the coefficient by covariates
names(postMode) <- covNames
names(approxPostStd) <- covNames
print('The posterior mode is:')
print(postMode)
print('The approximate posterior standard deviation is:')
print(approxPostStd)
# Compute an approximate 95% credible interval for the variable NSmallChild
# By simulation
data.NSmallChild <- as.data.frame(
mvtnorm::rmvnorm(n = 1000, mean = postMode, sigma = postCov)
)[,7]
# 95% credible interval
quantile(data.NSmallChild, probs = c(0.025,0.975))
CI_0025 = quantile(data.NSmallChild, probs = c(0.025,0.975))[1]
CI_0975 = quantile(data.NSmallChild, probs = c(0.025,0.975))[2]
# By mean and standard deviation
# 1.96 - T-Distribution
interval <- c(postMode[7] - 1.96 * approxPostStd[7], postMode[7] + 1.96 * approxPostStd[7])
names(interval) <- NULL
cat(interval)
###################################################################################
# Author: Mattias Villani, Linkoping University.
#         E-mail: mattias.villani@liu.se
#         web: http://mattiasvillani.com
# Script to illustrate numerical maximization of the Logistic or Probit regression
###################################################################################
###########   BEGIN USER INPUTS   ################
Probit <- 0           # If Probit <-0, then logistic model is used.
chooseCov <- c(1:16)  # Here we choose which covariates to include in the model
tau <- 10000;         # Prior scaling factor such that Prior Covariance = (tau^2)*I
###########     END USER INPUT    ################
#install.packages("mvtnorm") # Loading a package that contains the multivariate normal pdf
library("mvtnorm") # This command reads the mvtnorm package into R's memory. NOW we can use dmvnorm function.
# Loading data from file
Data<-read.table("SpamReduced.dat",header=TRUE)  # Spam data from Hastie et al.
# 1000 simulations of beta
data.hist <- as.data.frame(
mvtnorm::rmvnorm(n = 1000, mean = postMode, sigma = postCov)
)
colnames(data.hist) <- covNames
f <- function(cname){
ggplot(data.hist, aes_string(x=cname)) +
geom_histogram(aes(y = ..density..),
colour = "black",
fill   = "white",
bins   = 30) +
geom_density(alpha = .2, fill = "#FF6666")
}
plot(arrangeGrob(grobs = lapply(covNames, f)))
# the given values for the example
example <- matrix(c(1, 10, 8, 10, (10/10)^2, 40, 1, 1), nrow = 1)
sigmoid_function = function(x){
# this function returns create prediction from a regression (pred.hist) + usesed sigmoid function to
# create probabilites of that to some a classification problem (sigmoid)
#
# Arguments:
#   x = input to the function (your algorithm’s prediction e.g. mx + b)
#
# Returns:
#   data frame - s(x) = output between 0 and 1 (probability estimate) + flags
#
# Flags:
#   50% Decision Boundary
#
# Lecture 6 slide 4
# Classification with logistic regression - Logistic regression
# -------- prediction --------
# 1000 predictions for such example - pred.hist represents X
pred.hist <<- as.data.frame(t(example %*% t(data.hist)))
# comment:
# we used mvtnorm to simulate data.hist, so we use it at this point again
# instead of simulating again
# -------- sigmoid function --------
sigmoid <- as.data.frame(1/(1+exp(-pred.hist)))
colnames(sigmoid) <- "Probability"
# create flags
sigmoid$job_flag <- ifelse(sigmoid$Probability < 0.5, "no_job", "job")
sigmoid$job_flag_binary <- ifelse(sigmoid$Probability < 0.5, 0, 1)
# add simulation nr
sigmoid$nr <- c(1:nrow(sigmoid))
# return the output
sigmoid <<- sigmoid
#return(sigmoid)
}
sigmoid_function(example)
hist(sigmoid$job_flag_binary,2)
ggplot(data = sigmoid, aes(x = Probability)) +
geom_histogram(aes(y = ..density..),
colour = "black",
fill   = "white",
bins   = 30) +
geom_density(alpha = .2, fill = "#FF6666") +
ggtitle("Density - Histogram of work probabily") +
xlim(c(0,1))
ggplot(data = sigmoid, aes(x = sigmoid$Probability, y = sigmoid$job_flag_binary)) +
xlim(c(0,1)) + ylab("Classification") + xlab("Probability") +
ggtitle("Sigmoid function with 0.5 as decision boundary") +
geom_vline(aes(xintercept = 0.5, color = "#e67e22"), alpha = 0.5) +
geom_hline(aes(yintercept = 0.5, color = "#e67e22"), alpha = 0.5) +
stat_smooth(method="glm", aes(color = "Blue"), alpha = 0.5, se=FALSE, fullrange=TRUE,
method.args = list(family=binomial)) +
geom_point(aes(color = "#e74c3c")) +
scale_color_identity(name = "Colour",
breaks = c("#e74c3c","#e67e22", "Blue"),
labels = c("Observations", "Decision boundary" , "Logisic Regression"),
guide = "legend") +
theme(legend.position="bottom")
ggplot(pred.hist, aes(x=V1)) +
geom_density(alpha = .2, fill = "#FF6666") +
xlab("NSmallChild") + ggtitle("Density and credible interval of the feature NSmallChild ") +
geom_vline(aes(xintercept = CI_0975)) +
geom_vline(aes(xintercept = CI_0025))+
geom_text(aes(label ="97.5 % interval", x =  CI_0975 +0.3, y = 1),size = 3 )+
geom_text(aes(label ="2.5 % interval", x =  CI_0025 -0.3, y = 1),size = 3 )
pred.hist$V1
ggplot(pred.hist, aes(x=V1)) +
geom_density(alpha = .2, fill = "#FF6666") +
xlab("NSmallChild") + ggtitle("Density and credible interval of the feature NSmallChild ") +
geom_vline(aes(xintercept = CI_0950)) +
geom_vline(aes(xintercept = CI_0025))+
geom_text(aes(label ="97.5 % interval", x =  CI_0975 +0.3, y = 1),size = 3 )+
geom_text(aes(label ="2.5 % interval", x =  CI_0025 -0.3, y = 1),size = 3 )
